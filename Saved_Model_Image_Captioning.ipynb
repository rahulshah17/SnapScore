{"cells":[{"cell_type":"code","execution_count":null,"id":"46d2735d-ab4a-452e-bbdb-d0d28cef5568","metadata":{"id":"46d2735d-ab4a-452e-bbdb-d0d28cef5568","outputId":"33fb0ea5-efe3-461c-b7b2-f768a62319e8"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     /home/jdalal_umass_edu/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import os\n","from collections import Counter\n","\n","import numpy as np\n","import torch.utils.data\n","from torchvision import transforms\n","from torchvision import models\n","from torch.utils.data import DataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler, BatchSampler\n","from torchtext.data.metrics import bleu_score\n","from pycocotools.coco import COCO\n","\n","import math\n","import time\n","import pickle\n","import json\n","import os\n","import urllib\n","import zipfile\n","import random\n","from tqdm import tqdm\n","from copy import deepcopy\n","\n","\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"id":"1825ec3c-5b20-4de7-9a14-12dfc97721c9","metadata":{"id":"1825ec3c-5b20-4de7-9a14-12dfc97721c9"},"outputs":[],"source":["class EncoderCNN(nn.Module):\n","    def __init__(self, embed_size):\n","        super(EncoderCNN, self).__init__()\n","        # load the pre-trained ResNet\n","        resnet = models.resnet50(pretrained=True)\n","        # freeze the weights\n","        for param in resnet.parameters():\n","            param.requires_grad_(False)\n","        # grab all CNN layers except the last one\n","        modules = list(resnet.children())[:-1]\n","        self.resnet = nn.Sequential(*modules)\n","        # embedding layers\n","        self.embedding = nn.Linear(resnet.fc.in_features, embed_size)\n","\n","    def forward(self, images):\n","        # resnet stage\n","        features = self.resnet(images)\n","        # flatten to 1 dim\n","        features = features.view(features.size(0), -1)\n","        # embedding to final feature\n","        features = self.embedding(features)\n","        return features"]},{"cell_type":"code","execution_count":null,"id":"2beed2f4-cc82-4d22-808a-6f8688fbccc9","metadata":{"id":"2beed2f4-cc82-4d22-808a-6f8688fbccc9"},"outputs":[],"source":["class DecoderRNN(nn.Module):\n","    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n","        super(DecoderRNN, self).__init__()\n","        # embedding layer\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        # LSTM layer(s)\n","        self.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n","        # dense layer from hidden states to vocab dimension\n","        self.fc = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, features, captions):\n","        # batch size\n","        batch_size = features.shape[0]\n","        # embedding dimension\n","        embed_size = features.shape[1]\n","        # caption length\n","        seq_len = captions.shape[1]\n","        # remove the <end> token\n","        captions = captions[:, :-1]\n","        # pass the tokenized captions into the embedding layer\n","        embedded_captions = self.embedding(captions)  # (batch_size, seq_len-1, embed_size)\n","        # convert features as the very first tokens\n","        features = torch.unsqueeze(features, dim=1)  # (batch_size, 1, embed_size)\n","        # concatenate to obtain lstm_input\n","        lstm_input = torch.cat((features, embedded_captions), dim=1)  # (batch_size, seq_len, embed_size)\n","        # LSTM layer\n","        lstm_output, lstm_hidden = self.lstm(lstm_input)\n","        # dense layer\n","        fc_output = self.fc(lstm_output)\n","        return fc_output\n","\n","\n","    def sample(self, inputs, states=None, max_len=20):\n","        tokens = []\n","        x = inputs\n","        # output tokens one by one\n","        for _ in range(max_len):\n","            # lstm layer\n","            x, states = self.lstm(x, states)  # (batch_size=1, 1, hidden_size)\n","            # dense layer\n","            x = self.fc(x)  # (batch_size=1, 1, vocab_size)\n","            # token\n","            tok = torch.argmax(x, dim=-1)  # (batch_size=1, 1)\n","            # append to the output\n","            tokens.append(int(tok[0, 0]))\n","            # early stop (token == 1)\n","            if tok[0, 0] == 1:\n","                break\n","            # embedding\n","            x = self.embedding(tok)  # (batch_size, 1, embed_size)\n","        return tokens"]},{"cell_type":"code","execution_count":null,"id":"af26f3e1-fdbe-46a6-973d-73abfbb8de20","metadata":{"id":"af26f3e1-fdbe-46a6-973d-73abfbb8de20"},"outputs":[],"source":["def image_captioning_custom_image(img_path, encoder, decoder):\n","    encoder.eval()\n","    decoder.eval()\n","    encoder = encoder.to(device)\n","    decoder = decoder.to(device)\n","    # image preprocessing\n","    orig_image = np.array(Image.open(img_path).convert('RGB'))\n","    # plot the original image\n","    plt.imshow(orig_image)\n","    plt.axis('off')\n","\n","    # caption prediction\n","    image_t = transform_eval(Image.open(img_path).convert('RGB'))\n","    image_t = torch.unsqueeze(image_t, 0)\n","    image_t = image_t.to(device)\n","    with torch.no_grad():\n","        features_t = encoder(image_t).unsqueeze(1)\n","        token_list = decoder.sample(features_t)\n","    decoded_word_list, decoded_sentence = get_word_list_and_sentence(token_list)\n","    print(decoded_sentence)"]},{"cell_type":"code","execution_count":null,"id":"6493c98a-304f-4666-9267-d2d25a3a32a1","metadata":{"id":"6493c98a-304f-4666-9267-d2d25a3a32a1"},"outputs":[],"source":["EMBED_SIZE = 512\n","HIDDEN_SIZE = 512\n","VOCAB_SIZE = 8852\n","encoder = EncoderCNN(embed_size=EMBED_SIZE)\n","decoder = DecoderRNN(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=VOCAB_SIZE)"]},{"cell_type":"code","execution_count":null,"id":"ea29f17d-d36b-41c2-99a8-12c67fcee4f9","metadata":{"id":"ea29f17d-d36b-41c2-99a8-12c67fcee4f9"},"outputs":[],"source":["os.chdir('..')"]},{"cell_type":"code","execution_count":null,"id":"5e6e3eb0-e00a-4292-ba08-3d5189df161c","metadata":{"id":"5e6e3eb0-e00a-4292-ba08-3d5189df161c","outputId":"27ebc46e-7733-4d59-fe66-7226b6a7e315"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["model_name = \"020422\"\n","best_epoch = 1\n","\n","encoder.load_state_dict(torch.load(os.path.join(\"encoder_\" + model_name + \"_ep\" + str(best_epoch) + \".pth\")))\n","decoder.load_state_dict(torch.load(os.path.join(\"decoder_\" + model_name + \"_ep\" + str(best_epoch) + \".pth\")))"]},{"cell_type":"code","execution_count":null,"id":"d5b1e621-9f1e-4bcb-87d8-f117d1e223a3","metadata":{"id":"d5b1e621-9f1e-4bcb-87d8-f117d1e223a3"},"outputs":[],"source":["class Vocabulary(object):\n","    def __init__(self, vocab_threshold, vocab_file='/content/vocab.pkl',\n","                 start_word=\"<start>\", end_word=\"<end>\", unk_word=\"<unk>\",\n","                 annotations_file=\"cocoapi/annotations/captions_train2014.json\",\n","                 vocab_from_file=False):\n","        self.vocab_threshold = vocab_threshold\n","        self.vocab_file = vocab_file\n","        self.start_word = start_word\n","        self.end_word = end_word\n","        self.unk_word = unk_word\n","        self.annotations_file = annotations_file\n","        self.vocab_from_file = vocab_from_file\n","        self.get_vocab()\n","\n","    def get_vocab(self):\n","        # load and use the existing vocab file\n","        if os.path.exists(self.vocab_file) & self.vocab_from_file:\n","            with open(self.vocab_file, 'rb') as f:\n","                vocab = pickle.load(f)\n","                self.word2idx = vocab.word2idx\n","                self.idx2word = vocab.idx2word\n","            print('Vocabulary successfully loaded from vocab.pkl file!')\n","\n","        # build a new vocab file\n","        else:\n","            self.build_vocab()\n","            with open(self.vocab_file, 'wb') as f:\n","                pickle.dump(self, f)\n","\n","    def build_vocab(self):\n","        self.init_vocab()\n","        self.add_word(self.start_word)\n","        self.add_word(self.end_word)\n","        self.add_word(self.unk_word)\n","        self.add_captions()\n","\n","    def init_vocab(self):\n","        self.word2idx = {}\n","        self.idx2word = {}\n","        self.idx = 0\n","\n","    def add_word(self, word):\n","        if not word in self.word2idx:\n","            self.word2idx[word] = self.idx\n","            self.idx2word[self.idx] = word\n","            self.idx += 1\n","\n","    def add_captions(self):\n","        coco = COCO(self.annotations_file)\n","        counter = Counter()\n","        ids = coco.anns.keys()\n","        for i, id in enumerate(ids):\n","            caption = str(coco.anns[id]['caption'])\n","            tokens = nltk.tokenize.word_tokenize(caption.lower())\n","            counter.update(tokens)\n","\n","            if i % 100000 == 0:\n","                print(\"[%d/%d] Tokenizing captions...\" % (i, len(ids)))\n","\n","        words = [word for word, cnt in counter.items() if cnt >= self.vocab_threshold]\n","\n","        for i, word in enumerate(words):\n","            self.add_word(word)\n","\n","    def __call__(self, word):\n","        if not word in self.word2idx:\n","            return self.word2idx[self.unk_word]\n","        return self.word2idx[word]\n","\n","    def __len__(self):\n","        return len(self.word2idx)"]},{"cell_type":"code","execution_count":null,"id":"ff67fc01-fa63-4af3-b41b-d89a0cf15019","metadata":{"id":"ff67fc01-fa63-4af3-b41b-d89a0cf15019","outputId":"73a06a3f-14a0-437e-a378-9f0734eecba6"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading annotations into memory...\n","Done (t=0.58s)\n","creating index...\n","index created!\n","[0/414113] Tokenizing captions...\n","[100000/414113] Tokenizing captions...\n","[200000/414113] Tokenizing captions...\n","[300000/414113] Tokenizing captions...\n","[400000/414113] Tokenizing captions...\n"]}],"source":["VOCAB_THRESHOLD = 5\n","\n","# build vocab file from training data\n","train_vocab = Vocabulary(vocab_threshold=VOCAB_THRESHOLD,\n","                         vocab_file=\"./vocab.pkl\",\n","                         start_word=\"<start>\",\n","                         end_word=\"<end>\",\n","                         unk_word=\"<unk>\",\n","                         annotations_file=\"cocoapi/annotations/captions_train2014.json\",\n","                         vocab_from_file=False)"]},{"cell_type":"code","execution_count":null,"id":"9464c483-8095-4ce6-ab8c-2b386aa1ee9d","metadata":{"id":"9464c483-8095-4ce6-ab8c-2b386aa1ee9d"},"outputs":[],"source":["def get_word_list_and_sentence(token_list):\n","    word_list = []\n","\n","    for tok in token_list:\n","        # skip the <start> token\n","        if tok == 0:\n","            continue\n","\n","        # break if it's an <end> token\n","        if tok == 1:\n","            break\n","\n","        # look up the word\n","        word = train_vocab.idx2word[tok]\n","        word_list.append(word)\n","\n","    sentence = \" \".join(word_list)\n","\n","    return word_list, sentence"]},{"cell_type":"code","execution_count":null,"id":"309fce9a-a6e9-45a6-9f0f-89a95a28adfd","metadata":{"id":"309fce9a-a6e9-45a6-9f0f-89a95a28adfd","outputId":"3c167be6-e598-4fc2-d24d-d60084d59827"},"outputs":[{"name":"stdout","output_type":"stream","text":["a large bear is standing in the grass .\n"]}],"source":["# validation/test data transform\n","transform_eval = transforms.Compose([transforms.Resize(256),\n","                                     transforms.CenterCrop(224),\n","                                     transforms.ToTensor(),\n","                                     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","image_captioning_custom_image(\"./custom_test_images/animal_grass.png\", encoder, decoder)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}